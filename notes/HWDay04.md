# Day04 ä½œä¸š

## XTuneræ¦‚è¿°

- XTunerï¼šä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹å¾®è°ƒå·¥å…·ç®±ã€‚*ç”±* *MMRazor* *å’Œ* *MMDeploy* *è”åˆå¼€å‘ã€‚*
- ğŸ¤“ **å‚»ç“œåŒ–ï¼š** ä»¥ é…ç½®æ–‡ä»¶ çš„å½¢å¼å°è£…äº†å¤§éƒ¨åˆ†å¾®è°ƒåœºæ™¯ï¼Œ**0åŸºç¡€çš„éä¸“ä¸šäººå‘˜ä¹Ÿèƒ½ä¸€é”®å¼€å§‹å¾®è°ƒ**ã€‚
- ğŸƒ **è½»é‡çº§ï¼š** å¯¹äº 7B å‚æ•°é‡çš„LLMï¼Œ**å¾®è°ƒæ‰€éœ€çš„æœ€å°æ˜¾å­˜ä»…ä¸º 8GB** ï¼š **æ¶ˆè´¹çº§æ˜¾å¡âœ…ï¼Œcolabâœ…**

###  æ”¯æŒçš„å¼€æºLLM (2023.11.01)

- **[InternLM](https://huggingface.co/internlm/internlm-7b)** âœ…
- [Llamaï¼ŒLlama2](https://huggingface.co/meta-llama)
- [ChatGLM2](https://huggingface.co/THUDM/chatglm2-6b)ï¼Œ[ChatGLM3](https://huggingface.co/THUDM/chatglm3-6b-base)
- [Qwen](https://huggingface.co/Qwen/Qwen-7B)
- [Baichuan](https://huggingface.co/baichuan-inc/Baichuan-7B)ï¼Œ[Baichuan2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base)
- ......
- [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) 

### å¾®è°ƒåŸç†

- **LoRA** ï¼š**åªå¯¹ç©å…·ä¸­çš„æŸäº›é›¶ä»¶è¿›è¡Œæ”¹åŠ¨ï¼Œè€Œä¸æ˜¯å¯¹æ•´ä¸ªç©å…·è¿›è¡Œå…¨é¢æ”¹åŠ¨**ã€‚

- â€» è€Œ **QLoRA** æ˜¯ LoRA çš„ä¸€ç§æ”¹è¿›ï¼šå¦‚æœä½ æ‰‹é‡Œåªæœ‰ä¸€æŠŠç”Ÿé”ˆçš„èºä¸åˆ€ï¼Œä¹Ÿèƒ½æ”¹é€ ä½ çš„ç©å…·ã€‚

- **Full** :       ğŸ˜³ â†’ ğŸš²
- **[LoRA](http://arxiv.org/abs/2106.09685)** :     ğŸ˜³ â†’ ğŸ›µ
- **[QLoRA](http://arxiv.org/abs/2305.14314)** :   ğŸ˜³ â†’ ğŸ



## å¿«é€Ÿä¸Šæ‰‹

- å¹³å°ï¼šUbuntu + Anaconda + CUDA/CUDNN + 8GB nvidiaæ˜¾å¡

###  å®‰è£…

```
# InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch 2.0.1 çš„ç¯å¢ƒï¼š
/root/share/install_conda_env_internlm_base.sh xtuner0.1.9
# å…¶ä»–å¹³å°ï¼š
conda create --name xtuner0.1.9 python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate xtuner0.1.9
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
mkdir xtuner019 && cd xtuner019


# æ‹‰å– 0.1.9 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```

- å‡†å¤‡åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ internlm-7b-chat

```
# åˆ›å»ºä¸€ä¸ªå¾®è°ƒ oasst1 æ•°æ®é›†çš„å·¥ä½œè·¯å¾„ï¼Œè¿›å…¥
mkdir ~/ft-oasst1 && cd ~/ft-oasst1
```

### å¾®è°ƒ

#### å‡†å¤‡

- XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

```
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®
xtuner list-cfg
```

- å‡å¦‚æ˜¾ç¤ºbash: xtuner: command not foundçš„è¯å¯ä»¥è€ƒè™‘åœ¨ç»ˆç«¯è¾“å…¥ export PATH=$PATH:'/root/.local/bin'

æ‹·è´ä¸€ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š
`# xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}`

åœ¨æœ¬æ¡ˆä¾‹ä¸­å³ï¼šï¼ˆæ³¨æ„æœ€åæœ‰ä¸ªè‹±æ–‡å¥å·ï¼Œä»£è¡¨å¤åˆ¶åˆ°å½“å‰è·¯å¾„ï¼‰

```Bash
cd ~/ft-oasst1
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

é…ç½®æ–‡ä»¶åçš„è§£é‡Šï¼šxtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .

| æ¨¡å‹å         | internlm_chat_7b     |
| -------------- | -------------------- |
| ä½¿ç”¨ç®—æ³•       | qlora                |
| æ•°æ®é›†         | oasst1               |
| æŠŠæ•°æ®é›†è·‘å‡ æ¬¡ | è·‘3æ¬¡ï¼še3 (epoch 3 ) |

*æ—  chatæ¯”å¦‚ `internlm-7b` ä»£è¡¨æ˜¯åŸºåº§(base)æ¨¡å‹

#### æ¨¡å‹ä¸‹è½½

- æ–¹å¼ä¸€ã€æ•™å­¦å¹³å°ç›´æ¥å¤åˆ¶æ¨¡å‹ã€‚

```
cp -r /root/share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/
```

- æ–¹å¼äºŒã€ä¸ç”¨ xtuner é»˜è®¤çš„`ä» huggingface æ‹‰å–æ¨¡å‹`ï¼Œè€Œæ˜¯æå‰ä» ~~OpenXLab~~ ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°


```Bash
# åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œæ”¾æ¨¡å‹æ–‡ä»¶ï¼Œé˜²æ­¢æ•£è½ä¸€åœ°
mkdir ~/ft-oasst1/internlm-chat-7b

# è£…ä¸€ä¸‹æ‹‰å–æ¨¡å‹æ–‡ä»¶è¦ç”¨çš„åº“
pip install modelscope

# ä» modelscope ä¸‹è½½ä¸‹è½½æ¨¡å‹æ–‡ä»¶
cd ~/ft-oasst1
apt install git git-lfs -y
git lfs install
git lfs clone https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3
```

#### æ•°æ®é›†ä¸‹è½½

https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main

- ç”±äº huggingface ç½‘ç»œé—®é¢˜ï¼Œå’±ä»¬å·²ç»ç»™å¤§å®¶æå‰ä¸‹è½½å¥½äº†ï¼Œå¤åˆ¶åˆ°æ­£ç¡®ä½ç½®å³å¯ï¼š

```
cd ~/ft-oasst1
# ...-guanaco åé¢æœ‰ä¸ªç©ºæ ¼å’Œè‹±æ–‡å¥å·å•Š
cp -r /root/share/temp/datasets/openassistant-guanaco .
```

- å½“å‰è·¯å¾„çš„æ–‡ä»¶åº”è¯¥é•¿è¿™æ ·ï¼š

```bash
|-- internlm-chat-7b
|   |-- README.md
|   |-- config.json
|   |-- configuration.json
|   |-- configuration_internlm.py
|   |-- generation_config.json
|   |-- modeling_internlm.py
|   |-- pytorch_model-00001-of-00008.bin
|   |-- pytorch_model-00002-of-00008.bin
|   |-- pytorch_model-00003-of-00008.bin
|   |-- pytorch_model-00004-of-00008.bin
|   |-- pytorch_model-00005-of-00008.bin
|   |-- pytorch_model-00006-of-00008.bin
|   |-- pytorch_model-00007-of-00008.bin
|   |-- pytorch_model-00008-of-00008.bin
|   |-- pytorch_model.bin.index.json
|   |-- special_tokens_map.json
|   |-- tokenization_internlm.py
|   |-- tokenizer.model
|   `-- tokenizer_config.json
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
`-- openassistant-guanaco
    |-- openassistant_best_replies_eval.jsonl
    `-- openassistant_best_replies_train.jsonl
```

####  ä¿®æ”¹é…ç½®æ–‡ä»¶

ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ä¸º æœ¬åœ°è·¯å¾„

```bash
cd ~/ft-oasst1
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```

> åœ¨vimç•Œé¢å®Œæˆä¿®æ”¹åï¼Œè¯·è¾“å…¥:wqé€€å‡ºã€‚å‡å¦‚è®¤ä¸ºæ”¹é”™äº†å¯ä»¥ç”¨:q!é€€å‡ºä¸”ä¸ä¿å­˜ã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥è€ƒè™‘æ‰“å¼€pythonæ–‡ä»¶ç›´æ¥ä¿®æ”¹ï¼Œä½†æ³¨æ„ä¿®æ”¹å®Œåéœ€è¦æŒ‰ä¸‹Ctrl+Sè¿›è¡Œä¿å­˜ã€‚

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚

```diff
# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®é›†ä¸ºæœ¬åœ°è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = './openassistant-guanaco'
```

**å¸¸ç”¨è¶…å‚**

| å‚æ•°å              | è§£é‡Š                                                   |
| ------------------- | ------------------------------------------------------ |
| **data_path**       | æ•°æ®è·¯å¾„æˆ– HuggingFace ä»“åº“å                          |
| max_length          | å•æ¡æ•°æ®æœ€å¤§ Token æ•°ï¼Œè¶…è¿‡åˆ™æˆªæ–­                      |
| pack_to_max_length  | æ˜¯å¦å°†å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥åˆ° max_lengthï¼Œæé«˜ GPU åˆ©ç”¨ç‡     |
| accumulative_counts | æ¢¯åº¦ç´¯ç§¯ï¼Œæ¯å¤šå°‘æ¬¡ backward æ›´æ–°ä¸€æ¬¡å‚æ•°               |
| evaluation_inputs   | è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼šæ ¹æ®ç»™å®šçš„é—®é¢˜è¿›è¡Œæ¨ç†ï¼Œä¾¿äºè§‚æµ‹è®­ç»ƒçŠ¶æ€ |
| evaluation_freq     | Evaluation çš„è¯„æµ‹é—´éš” iter æ•°                          |
| ......              | ......                                                 |

> å¦‚æœæƒ³æŠŠæ˜¾å¡çš„ç°å­˜åƒæ»¡ï¼Œå……åˆ†åˆ©ç”¨æ˜¾å¡èµ„æºï¼Œå¯ä»¥å°† `max_length` å’Œ `batch_size` è¿™ä¸¤ä¸ªå‚æ•°è°ƒå¤§ã€‚

####  å¼€å§‹å¾®è°ƒ

**è®­ç»ƒï¼š**

xtuner train ${CONFIG_NAME_OR_PATH}

**ä¹Ÿå¯ä»¥å¢åŠ  deepspeed è¿›è¡Œè®­ç»ƒåŠ é€Ÿï¼š**

xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2


ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ QLoRA ç®—æ³•åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ InternLM-7Bï¼š

```Bash
# å•å¡
## ç”¨åˆšæ‰æ”¹å¥½çš„configæ–‡ä»¶è®­ç»ƒ
xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# å¤šå¡
NPROC_PER_NODE=${GPU_NUM} xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# è‹¥è¦å¼€å¯ deepspeed åŠ é€Ÿï¼Œå¢åŠ  --deepspeed deepspeed_zero2 å³å¯
```

> å¾®è°ƒå¾—åˆ°çš„ PTH æ¨¡å‹æ–‡ä»¶å’Œå…¶ä»–æ‚ä¸ƒæ‚å…«çš„æ–‡ä»¶éƒ½é»˜è®¤åœ¨å½“å‰çš„ `./work_dirs` ä¸­ã€‚

è·‘å®Œè®­ç»ƒåï¼Œå½“å‰è·¯å¾„åº”è¯¥é•¿è¿™æ ·ï¼š

```Bash
|-- internlm-chat-7b
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
|-- openassistant-guanaco
|   |-- openassistant_best_replies_eval.jsonl
|   `-- openassistant_best_replies_train.jsonl
`-- work_dirs
    `-- internlm_chat_7b_qlora_oasst1_e3_copy
        |-- 20231101_152923
        |   |-- 20231101_152923.log
        |   `-- vis_data
        |       |-- 20231101_152923.json
        |       |-- config.py
        |       `-- scalars.json
        |-- epoch_1.pth
        |-- epoch_2.pth
        |-- epoch_3.pth
        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py
        `-- last_checkpoint
```

#### å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œ**å³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹**

`xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH_file_dir} ${SAVE_PATH}`

åœ¨æœ¬ç¤ºä¾‹ä¸­ï¼Œä¸ºï¼š

```bash
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1

xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf
```

æ­¤æ—¶ï¼Œè·¯å¾„ä¸­åº”è¯¥é•¿è¿™æ ·ï¼š

```Bash
|-- internlm-chat-7b
|-- internlm_chat_7b_qlora_oasst1_e3_copy.py
|-- openassistant-guanaco
|   |-- openassistant_best_replies_eval.jsonl
|   `-- openassistant_best_replies_train.jsonl
|-- hf
|   |-- README.md
|   |-- adapter_config.json
|   |-- adapter_model.bin
|   `-- xtuner_config.py
`-- work_dirs
    `-- internlm_chat_7b_qlora_oasst1_e3_copy
        |-- 20231101_152923
        |   |-- 20231101_152923.log
        |   `-- vis_data
        |       |-- 20231101_152923.json
        |       |-- config.py
        |       `-- scalars.json
        |-- epoch_1.pth
        |-- epoch_2.pth
        |-- epoch_3.pth
        |-- internlm_chat_7b_qlora_oasst1_e3_copy.py
        `-- last_checkpoint
```

<span style="color: red;">**æ­¤æ—¶ï¼Œhf æ–‡ä»¶å¤¹å³ä¸ºæˆ‘ä»¬å¹³æ—¶æ‰€ç†è§£çš„æ‰€è°“ â€œLoRA æ¨¡å‹æ–‡ä»¶â€**</span>

> å¯ä»¥ç®€å•ç†è§£ï¼šLoRA æ¨¡å‹æ–‡ä»¶ = Adapter



### éƒ¨ç½²ä¸æµ‹è¯•

#### å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š

```Bash
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

####  ä¸åˆå¹¶åçš„æ¨¡å‹å¯¹è¯ï¼š

```Bash
# åŠ è½½ Adapter æ¨¡å‹å¯¹è¯ï¼ˆFloat 16ï¼‰
xtuner chat ./merged --prompt-template internlm_chat

# 4 bit é‡åŒ–åŠ è½½
# xtuner chat ./merged --bits 4 --prompt-template internlm_chat
```

####  Demo

- ä¿®æ”¹ `cli_demo.py` ä¸­çš„æ¨¡å‹è·¯å¾„

```diff
- model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"
+ model_name_or_path = "merged"
```

- è¿è¡Œ `cli_demo.py` ä»¥ç›®æµ‹å¾®è°ƒæ•ˆæœ

```bash
python ./cli_demo.py
```

**æ•ˆæœï¼š**

**`xtuner chat`** **çš„å¯åŠ¨å‚æ•°**

| å¯åŠ¨å‚æ•°              | å¹²å“ˆæ»´                                                       |
| --------------------- | ------------------------------------------------------------ |
| **--prompt-template** | æŒ‡å®šå¯¹è¯æ¨¡æ¿                                                 |
| --system              | æŒ‡å®šSYSTEMæ–‡æœ¬                                               |
| --system-template     | æŒ‡å®šSYSTEMæ¨¡æ¿                                               |
| -**-bits**            | LLMä½æ•°                                                      |
| --bot-name            | botåç§°                                                      |
| --with-plugins        | æŒ‡å®šè¦ä½¿ç”¨çš„æ’ä»¶                                             |
| **--no-streamer**     | æ˜¯å¦å¯ç”¨æµå¼ä¼ è¾“                                             |
| **--lagent**          | æ˜¯å¦ä½¿ç”¨lagent                                               |
| --command-stop-word   | å‘½ä»¤åœæ­¢è¯                                                   |
| --answer-stop-word    | å›ç­”åœæ­¢è¯                                                   |
| --offload-folder      | å­˜æ”¾æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼ˆæˆ–è€…å·²ç»å¸è½½æ¨¡å‹æƒé‡çš„æ–‡ä»¶å¤¹ï¼‰         |
| --max-new-tokens      | ç”Ÿæˆæ–‡æœ¬ä¸­å…è®¸çš„æœ€å¤§ `token` æ•°é‡                            |
| **--temperature**     | æ¸©åº¦å€¼                                                       |
| --top-k               | ä¿ç•™ç”¨äºé¡¶kç­›é€‰çš„æœ€é«˜æ¦‚ç‡è¯æ±‡æ ‡è®°æ•°                          |
| --top-p               | å¦‚æœè®¾ç½®ä¸ºå°äº1çš„æµ®ç‚¹æ•°ï¼Œä»…ä¿ç•™æ¦‚ç‡ç›¸åŠ é«˜äº `top_p` çš„æœ€å°ä¸€ç»„æœ€æœ‰å¯èƒ½çš„æ ‡è®° |
| --seed                | ç”¨äºå¯é‡ç°æ–‡æœ¬ç”Ÿæˆçš„éšæœºç§å­                                 |



## è‡ªå®šä¹‰å¾®è°ƒ

> ä»¥ **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†**ä¸ºä¾‹

### æ¦‚è¿°

#### **åœºæ™¯éœ€æ±‚**

   åŸºäº InternLM-chat-7B æ¨¡å‹ï¼Œç”¨ MedQA æ•°æ®é›†è¿›è¡Œå¾®è°ƒï¼Œå°†å…¶å¾€`åŒ»å­¦é—®ç­”`é¢†åŸŸå¯¹é½ã€‚

#### **çœŸå®æ•°æ®é¢„è§ˆ**

| é—®é¢˜                                                       | ç­”æ¡ˆ                                                         |
| ---------------------------------------------------------- | ------------------------------------------------------------ |
| What are ketorolac eye drops?ï¼ˆä»€ä¹ˆæ˜¯é…®å’¯é…¸æ»´çœ¼æ¶²ï¼Ÿï¼‰      | Ophthalmic   ketorolac is used to treat itchy eyes caused by allergies. It also is used to   treat swelling and redness (inflammation) that can occur after cataract   surgery. Ketorolac is in a class of medications called nonsteroidal   anti-inflammatory drugs (NSAIDs). It works by stopping the release of   substances that cause allergy symptoms and inflammation. |
| What medicines raise blood sugar? ï¼ˆä»€ä¹ˆè¯ç‰©ä¼šå‡é«˜è¡€ç³–ï¼Ÿï¼‰ | Some   medicines for conditions other than diabetes can raise your blood sugar   level. This is a concern when you have diabetes. Make sure every doctor you   see knows about all of the medicines, vitamins, or herbal supplements you   take. This means anything you take with or without a prescription. Examples include:     Barbiturates.     Thiazide diuretics.     Corticosteroids.     Birth control pills (oral contraceptives) and progesterone.     Catecholamines.     Decongestants that contain beta-adrenergic agents, such as pseudoephedrine.     The B vitamin niacin. The risk of high blood sugar from niacin lowers after you have taken it for a few months. The antipsychotic medicine olanzapine (Zyprexa). |

###  æ•°æ®å‡†å¤‡

> **ä»¥** **[Medication QA](https://github.com/abachaa/Medication_QA_MedInfo2019)** **æ•°æ®é›†ä¸ºä¾‹**

**åŸæ ¼å¼ï¼š(.xlsx)**

| **é—®é¢˜** | è¯ç‰©ç±»å‹ | é—®é¢˜ç±»å‹ | **å›ç­”** | ä¸»é¢˜ | URL  |
| -------- | -------- | -------- | -------- | ---- | ---- |
| aaa      | bbb      | ccc      | ddd      | eee  | fff  |

####  å°†æ•°æ®è½¬ä¸º XTuner çš„æ•°æ®æ ¼å¼

**ç›®æ ‡æ ¼å¼ï¼š(.jsonL)**

```JSON
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
```

ğŸ§ é€šè¿‡ pytho nè„šæœ¬ï¼šå°† `.xlsx` ä¸­çš„ é—®é¢˜ å’Œ å›ç­” ä¸¤åˆ— æå–å‡ºæ¥ï¼Œå†æ”¾å…¥ `.jsonL` æ–‡ä»¶çš„æ¯ä¸ª conversation çš„ input å’Œ output ä¸­ã€‚

> è¿™ä¸€æ­¥çš„ python è„šæœ¬å¯ä»¥è¯· ChatGPT æ¥å®Œæˆã€‚

```text
Write a python file for me. using openpyxl. input file name is MedQA2019.xlsx
Step1: The input file is .xlsx. Exact the column A and column D in the sheet named "DrugQA" .
Step2: Put each value in column A into each "input" of each "conversation". Put each value in column D into each "output" of each "conversation".
Step3: The output file is .jsonL. It looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step4: All "system" value changes to "You are a professional, highly experienced doctor professor. You always provide accurate, comprehensive, and detailed answers based on the patients' questions."
```

> ChatGPT ç”Ÿæˆçš„ python ä»£ç è§æœ¬ä»“åº“çš„ [xlsx2jsonl.py](./xlsx2jsonl.py)


æ‰§è¡Œ python è„šæœ¬ï¼Œè·å¾—æ ¼å¼åŒ–åçš„æ•°æ®é›†ï¼š

```bash
python xlsx2jsonl.py
```

**æ ¼å¼åŒ–åçš„æ•°æ®é›†é•¿è¿™æ ·ï¼š**

æ­¤æ—¶ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥å¯¹æ•°æ®è¿›è¡Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ†å‰²ï¼ŒåŒæ ·å¯ä»¥è®© ChatGPT å†™ python ä»£ç ã€‚å½“ç„¶å¦‚æœä½ æ²¡æœ‰ä¸¥æ ¼çš„ç§‘ç ”éœ€æ±‚ã€ä¸åœ¨ä¹â€œè®­ç»ƒé›†æ³„éœ²â€çš„é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä¸åšè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„åˆ†å‰²ã€‚

####  åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†

```text
my .jsonL file looks like:
[{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
},
{
    "conversation":[
        {
            "system": "xxx",
            "input": "xxx",
            "output": "xxx"
        }
    ]
}]
Step1, read the .jsonL file.
Step2, count the amount of the "conversation" elements.
Step3, randomly split all "conversation" elements by 7:3. Targeted structure is same as the input.
Step4, save the 7/10 part as train.jsonl. save the 3/10 part as test.jsonl
```

ç”Ÿæˆçš„pythonä»£ç è§ [split2train_and_test.py](./split2train_and_test.py)

###  å¼€å§‹è‡ªå®šä¹‰å¾®è°ƒ

æ­¤æ—¶ï¼Œæˆ‘ä»¬é‡æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æ¥ç©â€œå¾®è°ƒè‡ªå®šä¹‰æ•°æ®é›†â€

```bash
mkdir ~/ft-medqa && cd ~/ft-medqa
```

æŠŠå‰é¢ä¸‹è½½å¥½çš„internlm-chat-7bæ¨¡å‹æ–‡ä»¶å¤¹æ‹·è´è¿‡æ¥ã€‚

```bash
cp -r ~/ft-oasst1/internlm-chat-7b .
```

åˆ«å¿˜äº†æŠŠè‡ªå®šä¹‰æ•°æ®é›†ï¼Œå³å‡ ä¸ª `.jsonL`ï¼Œä¹Ÿä¼ åˆ°æœåŠ¡å™¨ä¸Šã€‚

```bash
git clone https://github.com/InternLM/tutorial
```

```bash
cp ~/tutorial/xtuner/MedQA2019-structured-train.jsonl .
```



####  å‡†å¤‡é…ç½®æ–‡ä»¶

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
# æ”¹ä¸ªæ–‡ä»¶å
mv internlm_chat_7b_qlora_oasst1_e3_copy.py internlm_chat_7b_qlora_medqa2019_e3.py

# ä¿®æ”¹é…ç½®æ–‡ä»¶å†…å®¹
vim internlm_chat_7b_qlora_medqa2019_e3.py
```

å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚

```diff
# ä¿®æ”¹importéƒ¨åˆ†
- from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.dataset.map_fns import template_map_fn_factory

# ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'

# ä¿®æ”¹è®­ç»ƒæ•°æ®ä¸º MedQA2019-structured-train.jsonl è·¯å¾„
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'MedQA2019-structured-train.jsonl'

# ä¿®æ”¹ train_dataset å¯¹è±¡
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=alpaca_map_fn,
+   dataset_map_fn=None,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)
```

####  **XTunerï¼å¯åŠ¨ï¼**

```
xtuner train internlm_chat_7b_qlora_medqa2019_e3.py --deepspeed deepspeed_zero2
```

#### pth è½¬ huggingface

åŒå‰è¿°ï¼Œè¿™é‡Œä¸èµ˜è¿°äº†ã€‚[å°†å¾—åˆ°çš„-pth-æ¨¡å‹è½¬æ¢ä¸º-huggingface-æ¨¡å‹å³ç”Ÿæˆadapteræ–‡ä»¶å¤¹](#236-å°†å¾—åˆ°çš„-pth-æ¨¡å‹è½¬æ¢ä¸º-huggingface-æ¨¡å‹å³ç”Ÿæˆadapteræ–‡ä»¶å¤¹)  

#### éƒ¨ç½²ä¸æµ‹è¯•

åŒå‰è¿°ã€‚[éƒ¨ç½²ä¸æµ‹è¯•](#24-éƒ¨ç½²ä¸æµ‹è¯•)

## ã€è¡¥å……ã€‘ç”¨ MS-Agent æ•°æ®é›† èµ‹äºˆ LLM ä»¥ Agent èƒ½åŠ›

###  æ¦‚è¿°

MSAgent æ•°æ®é›†æ¯æ¡æ ·æœ¬åŒ…å«ä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼ˆconversationsï¼‰ï¼Œå…¶é‡Œé¢åŒ…å«äº† systemã€userã€assistant ä¸‰ç§å­—æ®µã€‚å…¶ä¸­ï¼š

- system: è¡¨ç¤ºç»™æ¨¡å‹å‰ç½®çš„äººè®¾è¾“å…¥ï¼Œå…¶ä¸­æœ‰å‘Šè¯‰æ¨¡å‹å¦‚ä½•è°ƒç”¨æ’ä»¶ä»¥åŠç”Ÿæˆè¯·æ±‚

- user: è¡¨ç¤ºç”¨æˆ·çš„è¾“å…¥ promptï¼Œåˆ†ä¸ºä¸¤ç§ï¼Œé€šç”¨ç”Ÿæˆçš„promptå’Œè°ƒç”¨æ’ä»¶éœ€æ±‚çš„ prompt

- assistant: ä¸ºæ¨¡å‹çš„å›å¤ã€‚å…¶ä¸­ä¼šåŒ…æ‹¬æ’ä»¶è°ƒç”¨ä»£ç å’Œæ‰§è¡Œä»£ç ï¼Œè°ƒç”¨ä»£ç æ˜¯è¦ LLM ç”Ÿæˆçš„ï¼Œè€Œæ‰§è¡Œä»£ç æ˜¯è°ƒç”¨æœåŠ¡æ¥ç”Ÿæˆç»“æœçš„

ä¸€æ¡è°ƒç”¨ç½‘é¡µæœç´¢æ’ä»¶æŸ¥è¯¢â€œä¸Šæµ·æ˜å¤©å¤©æ°”â€çš„æ•°æ®æ ·æœ¬ç¤ºä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

###  å¾®è°ƒæ­¥éª¤

#### å‡†å¤‡å·¥ä½œ

> xtuner æ˜¯ä»å›½å†…çš„ ModelScope å¹³å°ä¸‹è½½ MS-Agent æ•°æ®é›†ï¼Œå› æ­¤ä¸ç”¨æå‰æ‰‹åŠ¨ä¸‹è½½æ•°æ®é›†æ–‡ä»¶ã€‚

```bash
# å‡†å¤‡å·¥ä½œ
mkdir ~/ft-msagent && cd ~/ft-msagent
cp -r ~/ft-oasst1/internlm-chat-7b .

# æŸ¥çœ‹é…ç½®æ–‡ä»¶
xtuner list-cfg | grep msagent

# å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•
xtuner copy-cfg internlm_7b_qlora_msagent_react_e3_gpu8 .

# ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
vim ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py 
```

```diff
- pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
+ pretrained_model_name_or_path = './internlm-chat-7b'
```

####  å¼€å§‹å¾®è°ƒ

```Bash
xtuner train ./internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2
```

###  ç›´æ¥ä½¿ç”¨

> ç”±äº msagent çš„è®­ç»ƒéå¸¸è´¹æ—¶ï¼Œå¤§å®¶å¦‚æœæƒ³å°½å¿«æŠŠè¿™ä¸ªæ•™ç¨‹è·Ÿå®Œï¼Œå¯ä»¥ç›´æ¥ä» modelScope æ‹‰å–å’±ä»¬å·²ç»å¾®è°ƒå¥½äº†çš„ Adapterã€‚å¦‚ä¸‹æ¼”ç¤ºã€‚

####  ä¸‹è½½ Adapter

```Bash
cd ~/ft-msagent
apt install git git-lfs
git lfs install
git lfs clone https://www.modelscope.cn/xtuner/internlm-7b-qlora-msagent-react.git
```

OKï¼Œç°åœ¨ç›®å½•åº”è¯¥é•¿è¿™æ ·ï¼š

- internlm_7b_qlora_msagent_react_e3_gpu8_copy.py
- internlm-7b-qlora-msagent-react
- internlm-chat-7b
- work_dirï¼ˆå¯æœ‰å¯æ— ï¼‰

æœ‰äº†è¿™ä¸ªåœ¨ msagent ä¸Šè®­ç»ƒå¾—åˆ°çš„Adapterï¼Œæ¨¡å‹ç°åœ¨å·²ç»æœ‰ agent èƒ½åŠ›äº†ï¼å°±å¯ä»¥åŠ  --lagent ä»¥è°ƒç”¨æ¥è‡ª lagent çš„ä»£ç†åŠŸèƒ½äº†ï¼

#### æ·»åŠ  serper ç¯å¢ƒå˜é‡

> **å¼€å§‹ chat ä¹‹å‰ï¼Œè¿˜è¦åŠ ä¸ª serper çš„ç¯å¢ƒå˜é‡ï¼š**
>
> å» serper.dev å…è´¹æ³¨å†Œä¸€ä¸ªè´¦å·ï¼Œç”Ÿæˆè‡ªå·±çš„ api keyã€‚è¿™ä¸ªä¸œè¥¿æ˜¯ç”¨æ¥ç»™ lagent å»è·å– google æœç´¢çš„ç»“æœçš„ã€‚ç­‰äºæ˜¯ serper.dev å¸®ä½ å»è®¿é—® googleï¼Œè€Œä¸æ˜¯ä»ä½ è‡ªå·±æœ¬åœ°å»è®¿é—® google äº†ã€‚

æ·»åŠ  serper api key åˆ°ç¯å¢ƒå˜é‡ï¼š

```bash
export SERPER_API_KEY=abcdefg
```

####  xtuner + agentï¼Œå¯åŠ¨ï¼

```bash
xtuner chat ./internlm-chat-7b --adapter internlm-7b-qlora-msagent-react --lagent
```

#### æŠ¥é”™å¤„ç†

xtuner chat å¢åŠ  --lagent å‚æ•°åï¼ŒæŠ¥é”™ ```TypeError: transfomers.modelsauto.auto factory. BaseAutoModelClass.from pretrained() got multiple values for keyword argument "trust remote code"```	

æ³¨é‡Šæ‰å·²å®‰è£…åŒ…ä¸­çš„ä»£ç ï¼š

```bash
vim /root/xtuner019/xtuner/xtuner/tools/chat.py
```

- é—®é¢˜ï¼šhttps://docs.qq.com/doc/DY1d2ZVFlbXlrUERj

##  æ³¨æ„äº‹é¡¹

æœ¬æ•™ç¨‹ä½¿ç”¨ xtuner 0.1.9 ç‰ˆæœ¬

```
torch                         2.1.1
transformers                  4.34.0
transformers-stream-generator 0.0.4
```

```bash
pip install torch==2.1.1
pip install transformers==4.34.0
pip install transformers-stream-generator=0.0.4
```

CUDA ç›¸å…³ï¼š

```
NVIDIA-SMI 535.54.03              
Driver Version: 535.54.03    
CUDA Version: 12.2

nvidia-cuda-cupti-cu12        12.1.105
nvidia-cuda-nvrtc-cu12        12.1.105
nvidia-cuda-runtime-cu12      12.1.105
```

## XTuner InternLM-Chat ä¸ªäººå°åŠ©æ‰‹è®¤çŸ¥å¾®è°ƒå®è·µ

### å¾®è°ƒç¯å¢ƒå‡†å¤‡

```
# InternStudio å¹³å°ä¸­ï¼Œä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch 2.0.1 çš„ç¯å¢ƒï¼ˆåç»­å‡åœ¨è¯¥ç¯å¢ƒæ‰§è¡Œï¼Œè‹¥ä¸ºå…¶ä»–ç¯å¢ƒå¯ä½œä¸ºå‚è€ƒï¼‰
# è¿›å…¥ç¯å¢ƒåé¦–å…ˆ bash
bash
conda create --name personal_assistant --clone=/root/share/conda_envs/internlm-base
# å¦‚æœåœ¨å…¶ä»–å¹³å°ï¼š
# conda create --name personal_assistant python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate personal_assistant
# è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
cd ~
# åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
# personal_assistantç”¨äºå­˜æ”¾æœ¬æ•™ç¨‹æ‰€ä½¿ç”¨çš„ä¸œè¥¿
mkdir /root/personal_assistant && cd /root/personal_assistant
mkdir /root/personal_assistant/xtuner019 && cd /root/personal_assistant/xtuner019

# æ‹‰å– 0.1.9 çš„ç‰ˆæœ¬æºç 
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# è¿›å…¥æºç ç›®å½•
cd xtuner

# ä»æºç å®‰è£… XTuner
pip install -e '.[all]'
```

- åˆ›å»º`data`æ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾ç”¨äºè®­ç»ƒçš„æ•°æ®é›†

```bash
mkdir -p /root/personal_assistant/data && cd /root/personal_assistant/data
```

åœ¨`data`ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªjsonæ–‡ä»¶`personal_assistant.json`ä½œä¸ºæœ¬æ¬¡å¾®è°ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ã€‚jsonä¸­å†…å®¹å¯å‚è€ƒä¸‹æ–¹(å¤åˆ¶ç²˜è´´næ¬¡åšæ•°æ®å¢å¹¿ï¼Œæ•°æ®é‡å°æ— æ³•æœ‰æ•ˆå¾®è°ƒï¼Œä¸‹é¢ä»…ç”¨äºå±•ç¤ºæ ¼å¼ï¼Œä¸‹é¢ä¹Ÿæœ‰ç”Ÿæˆè„šæœ¬)

å…¶ä¸­`conversation`è¡¨ç¤ºä¸€æ¬¡å¯¹è¯çš„å†…å®¹ï¼Œ`input`ä¸ºè¾“å…¥ï¼Œå³ç”¨æˆ·ä¼šé—®çš„é—®é¢˜ï¼Œ`output`ä¸ºè¾“å‡ºï¼Œå³æƒ³è¦æ¨¡å‹å›ç­”çš„ç­”æ¡ˆã€‚

```json
[
    {
        "conversation": [
            {
                "input": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                "output": "æˆ‘æ˜¯å°ç…¤çƒï¼Œæ˜¯å­™å°åŒ—çš„ä¸ªäººå°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦"
            }
        ]
    },
    {
        "conversation": [
            {
                "input": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»",
                "output": "æˆ‘æ˜¯å°ç…¤çƒï¼Œæ˜¯å­™å°åŒ—çš„ä¸ªäººå°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦"
            }
        ]
    }
]
```

ä»¥ä¸‹æ˜¯ä¸€ä¸ªpythonè„šæœ¬ï¼Œç”¨äºç”Ÿæˆæ•°æ®é›†ã€‚åœ¨`data`ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªgenerate_data.pyæ–‡ä»¶ï¼Œå°†ä»¥ä¸‹ä»£ç å¤åˆ¶è¿›å»ï¼Œç„¶åè¿è¡Œè¯¥è„šæœ¬å³å¯ç”Ÿæˆæ•°æ®é›†ã€‚

```python
import json

# è¾“å…¥ä½ çš„åå­—
name = 'Shengshenlan'
# é‡å¤æ¬¡æ•°
n = 10000

data = [
    {
        "conversation": [
            {
                "input": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»",
                "output": "æˆ‘æ˜¯{}çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦".format(name)
            }
        ]
    }
]

for i in range(n):
    data.append(data[0])

with open('personal_assistant.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)

```

### é…ç½®å‡†å¤‡

ä¸‹è½½æ¨¡å‹`InternLM-chat-7B`

[InternStudio](https://studio.intern-ai.org.cn/) å¹³å°çš„ `share` ç›®å½•ä¸‹å·²ç»ä¸ºæˆ‘ä»¬å‡†å¤‡äº†å…¨ç³»åˆ—çš„ `InternLM` æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¤åˆ¶`internlm-chat-7b`ï¼š

```bash
mkdir -p /root/personal_assistant/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/personal_assistant/model/Shanghai_AI_Laboratory
```

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

```bash
# åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®
xtuner list-cfg
```



```bash
#åˆ›å»ºç”¨äºå­˜æ”¾é…ç½®çš„æ–‡ä»¶å¤¹configå¹¶è¿›å…¥
mkdir /root/personal_assistant/config && cd /root/personal_assistant/config
```

æ‹·è´ä¸€ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š`xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}`
åœ¨æœ¬ä¾‹ä¸­ï¼šï¼ˆæ³¨æ„æœ€åæœ‰ä¸ªè‹±æ–‡å¥å·ï¼Œä»£è¡¨å¤åˆ¶åˆ°å½“å‰è·¯å¾„ï¼‰

```bash
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

ä¿®æ”¹æ‹·è´åçš„æ–‡ä»¶internlm_chat_7b_qlora_oasst1_e3_copy.pyï¼Œä¿®æ”¹ä¸‹è¿°ä½ç½®ï¼š
(è¿™æ˜¯ä¸€ä»½ä¿®æ”¹å¥½çš„æ–‡ä»¶[internlm_chat_7b_qlora_oasst1_e3_copy.py](./internlm_chat_7b_qlora_oasst1_e3_copy.py))

```bash
# PART 1 ä¸­
# é¢„è®­ç»ƒæ¨¡å‹å­˜æ”¾çš„ä½ç½®
pretrained_model_name_or_path = '/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b'

# å¾®è°ƒæ•°æ®å­˜æ”¾çš„ä½ç½®
data_path = '/root/personal_assistant/data/personal_assistant.json'

# è®­ç»ƒä¸­æœ€å¤§çš„æ–‡æœ¬é•¿åº¦
max_length = 512

# æ¯ä¸€æ‰¹è®­ç»ƒæ ·æœ¬çš„å¤§å°
batch_size = 2

# æœ€å¤§è®­ç»ƒè½®æ•°
max_epochs = 3

# éªŒè¯çš„é¢‘ç‡
evaluation_freq = 90

# ç”¨äºè¯„ä¼°è¾“å‡ºå†…å®¹çš„é—®é¢˜ï¼ˆç”¨äºè¯„ä¼°çš„é—®é¢˜å°½é‡ä¸æ•°æ®é›†çš„questionä¿æŒä¸€è‡´ï¼‰
evaluation_inputs = [ 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»' ]


# PART 3 ä¸­
dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path))
dataset_map_fn=None
```

### å¾®è°ƒå¯åŠ¨

ç”¨`xtuner train`å‘½ä»¤å¯åŠ¨è®­ç»ƒã€

```bash
xtuner train /root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py
```

>ä¼šåœ¨è®­ç»ƒå®Œæˆåï¼Œè¾“å‡ºç”¨äºéªŒè¯çš„Sample output

### å¾®è°ƒåå‚æ•°è½¬æ¢/åˆå¹¶

è®­ç»ƒåçš„pthæ ¼å¼å‚æ•°è½¬Hugging Faceæ ¼å¼

```bash
# åˆ›å»ºç”¨äºå­˜æ”¾Hugging Faceæ ¼å¼å‚æ•°çš„hfæ–‡ä»¶å¤¹
mkdir /root/personal_assistant/config/work_dirs/hf

export MKL_SERVICE_FORCE_INTEL=1

# é…ç½®æ–‡ä»¶å­˜æ”¾çš„ä½ç½®
export CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py

# æ¨¡å‹è®­ç»ƒåå¾—åˆ°çš„pthæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
export PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth

# pthæ–‡ä»¶è½¬æ¢ä¸ºHugging Faceæ ¼å¼åå‚æ•°å­˜æ”¾çš„ä½ç½®
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf

# æ‰§è¡Œå‚æ•°è½¬æ¢
xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH
```

Mergeæ¨¡å‹å‚æ•°

```bash
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER='GNU'

# åŸå§‹æ¨¡å‹å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b

# Hugging Faceæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
export NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf

# æœ€ç»ˆMergeåçš„å‚æ•°å­˜æ”¾çš„ä½ç½®
mkdir /root/personal_assistant/config/work_dirs/hf_merge
export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge

# æ‰§è¡Œå‚æ•°Merge
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```

### ç½‘é¡µDEMO

å®‰è£…ç½‘é¡µDemoæ‰€éœ€ä¾èµ–

```bash
pip install streamlit==1.24.0
```

ä¸‹è½½[InternLM](https://studio.intern-ai.org.cn/)é¡¹ç›®ä»£ç ï¼ˆæ¬¢è¿Starï¼‰

```bash
# åˆ›å»ºcodeæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾InternLMé¡¹ç›®ä»£ç 
mkdir /root/personal_assistant/code && cd /root/personal_assistant/code
git clone https://github.com/InternLM/InternLM.git
```

å°† `/root/code/InternLM/web_demo.py` ä¸­ 29 è¡Œå’Œ 33 è¡Œçš„æ¨¡å‹è·¯å¾„æ›´æ¢ä¸ºMergeåå­˜æ”¾å‚æ•°çš„è·¯å¾„ `/root/personal_assistant/config/work_dirs/hf_merge`

è¿è¡Œ `/root/personal_assistant/code/InternLM` ç›®å½•ä¸‹çš„ `web_demo.py` æ–‡ä»¶ï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤åï¼Œ[**æŸ¥çœ‹æœ¬æ•™ç¨‹5.2é…ç½®æœ¬åœ°ç«¯å£å**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨è¾“å…¥ `http://127.0.0.1:6006` å³å¯ã€‚

```
streamlit run /root/personal_assistant/code/InternLM/web_demo.py --server.address 127.0.0.1 --server.port 6066
```

æ³¨æ„ï¼šè¦åœ¨æµè§ˆå™¨æ‰“å¼€ `http://127.0.0.1:6066` é¡µé¢åï¼Œæ¨¡å‹æ‰ä¼šåŠ è½½ã€‚
åœ¨åŠ è½½å®Œæ¨¡å‹ä¹‹åï¼Œå°±å¯ä»¥ä¸å¾®è°ƒåçš„ InternLM-Chat-7B è¿›è¡Œå¯¹è¯äº†

## ä½œä¸š

### **åŸºç¡€ä½œä¸šï¼š**

æ„å»ºæ•°æ®é›†ï¼Œä½¿ç”¨ XTuner å¾®è°ƒ InternLM-Chat-7B æ¨¡å‹, è®©æ¨¡å‹å­¦ä¹ åˆ°å®ƒæ˜¯ä½ çš„æ™ºèƒ½å°åŠ©æ‰‹ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬ä½œä¸šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„è¾“å‡ºéœ€è¦**å°†ä¸è¦è‘±å§œè’œå¤§ä½¬**æ›¿æ¢æˆè‡ªå·±åå­—æˆ–æ˜µç§°ï¼

- å¾®è°ƒDemo(äº”å°æ—¶è¿˜æ²¡å®Œæˆï¼‰ï¼šå­˜å‚¨ç©ºé—´è¶…äº†ï¼Œä¸å¤Ÿç”¨å•Š

![image-20240119182916060](assets/HWDay04/image-20240119182916060.png)

- ä½œä¸šç»“æœå¾®è°ƒå‰

![image-20240119192532238](assets/HWDay04/image-20240119192532238.png)

- å¾®è°ƒå

![image-20240119195756537](assets/HWDay04/image-20240119195756537.png)

![image-20240119203910980](assets/HWDay04/image-20240119203910980.png)

- ç•Œé¢æŠ¥é”™ï¼ˆéœ€è¦è¿›å…¥InternLMå¯åŠ¨ï¼‰

![image-20240119205739755](assets/HWDay04/image-20240119205739755.png)

- è°ƒæ•´åå®Œæˆ

![image-20240119212237469](assets/HWDay04/image-20240119212237469.png)

### è¿›é˜¶ä½œä¸š:

- å°†è®­ç»ƒå¥½çš„Adapteræ¨¡å‹æƒé‡ä¸Šä¼ åˆ° OpenXLab

![image-20240120163503269](assets/HWDay04/image-20240120163503269.png)

- æä¸ªå°å»ºè®®ï¼šhttps://openxlab.org.cn/home  ä¸é€‚é…360æé€Ÿæµè§ˆå™¨ï¼Œä¹±æ‰äº†

![image-20240120110036268](assets/HWDay04/image-20240120110036268.png)

